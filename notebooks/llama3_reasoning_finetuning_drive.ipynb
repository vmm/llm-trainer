{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama 3 for Reasoning with QLoRA (Drive-Integrated)\n",
    "\n",
    "This notebook demonstrates fine-tuning Llama 3 8B using QLoRA for improved reasoning capabilities, with all data saved to Google Drive for persistence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, check GPU availability, install dependencies, and set up persistent storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/vmm/llm-trainer.git\n",
    "%cd llm-trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix module import issues\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check and fix the working directory\n",
    "if not os.path.exists('src'):\n",
    "    # If we're not in the repo root, try to find it\n",
    "    if os.path.exists('llm-trainer'):\n",
    "        %cd llm-trainer\n",
    "    else:\n",
    "        # If we can't find it, raise an error\n",
    "        raise FileNotFoundError(\"Cannot find repository root directory with 'src' folder\")\n",
    "\n",
    "# Add the current directory to Python's path\n",
    "sys.path.append('.')\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Python path includes current directory: {'./' in sys.path or '.' in sys.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Configure output directory in Google Drive (change this to your preferred location)\n",
    "DRIVE_OUTPUT_DIR = \"llm-trainer-output\"  # Will be created under /content/drive/MyDrive/\n",
    "\n",
    "# You can change this to a different name if you want, e.g.,:\n",
    "# DRIVE_OUTPUT_DIR = \"my-llm-experiments/llama3-reasoning-1\"\n",
    "\n",
    "# Full path to the output directory\n",
    "DRIVE_BASE_PATH = f\"/content/drive/MyDrive/{DRIVE_OUTPUT_DIR}\"\n",
    "\n",
    "# Specific paths for different components\n",
    "DRIVE_DATASET_PATH = f\"{DRIVE_BASE_PATH}/datasets/natural_reasoning_processed\"\n",
    "DRIVE_MODEL_PATH = f\"{DRIVE_BASE_PATH}/models/llama3_reasoning\"\n",
    "DRIVE_EVAL_PATH = f\"{DRIVE_BASE_PATH}/evaluation/reasoning_results\"\n",
    "DRIVE_ADAPTER_PATH = f\"{DRIVE_BASE_PATH}/lora_adapter\"\n",
    "DRIVE_ADAPTER_ZIP = f\"{DRIVE_BASE_PATH}/lora_adapter.zip\"\n",
    "\n",
    "# Create project directories in Drive\n",
    "!mkdir -p {DRIVE_BASE_PATH}/datasets\n",
    "!mkdir -p {DRIVE_BASE_PATH}/models\n",
    "!mkdir -p {DRIVE_BASE_PATH}/evaluation\n",
    "!mkdir -p {DRIVE_BASE_PATH}/logs\n",
    "\n",
    "print(f\"All outputs will be saved to Google Drive under: {DRIVE_BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to perform final backup of all training artifacts\n",
    "def backup_all_training_artifacts():\n",
    "    \"\"\"\n",
    "    Perform a complete backup of all training artifacts to Google Drive.\n",
    "    This should be called at the end of training or when you want to ensure\n",
    "    everything is saved before shutting down the Colab instance.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import time\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"PERFORMING FINAL BACKUP TO GOOGLE DRIVE\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "    \n",
    "    # Create all required directories\n",
    "    os.makedirs(f\"{DRIVE_BASE_PATH}/models\", exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_BASE_PATH}/datasets\", exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_BASE_PATH}/logs\", exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_BASE_PATH}/evaluation\", exist_ok=True)\n",
    "    \n",
    "    # Check for local output directory\n",
    "    if os.path.exists('output'):\n",
    "        # Copy all outputs (models, checkpoints, logs)\n",
    "        !rsync -av --progress output/ {DRIVE_BASE_PATH}/models/ 2>/dev/null || cp -r output/* {DRIVE_BASE_PATH}/models/ 2>/dev/null || true\n",
    "        print(f\"✓ Backed up output directory to {DRIVE_BASE_PATH}/models/\")\n",
    "    \n",
    "    # Check for local data directory \n",
    "    if os.path.exists('data'):\n",
    "        # Copy all datasets\n",
    "        !rsync -av --progress data/ {DRIVE_BASE_PATH}/datasets/ 2>/dev/null || cp -r data/* {DRIVE_BASE_PATH}/datasets/ 2>/dev/null || true\n",
    "        print(f\"✓ Backed up data directory to {DRIVE_BASE_PATH}/datasets/\")\n",
    "    \n",
    "    # Check for local logs\n",
    "    if os.path.exists('logs'):\n",
    "        # Copy all logs\n",
    "        !rsync -av --progress logs/ {DRIVE_BASE_PATH}/logs/ 2>/dev/null || cp -r logs/* {DRIVE_BASE_PATH}/logs/ 2>/dev/null || true\n",
    "        print(f\"✓ Backed up logs directory to {DRIVE_BASE_PATH}/logs/\")\n",
    "    \n",
    "    # Check for local evaluation results\n",
    "    if os.path.exists('evaluation'):\n",
    "        # Copy all evaluation results\n",
    "        !rsync -av --progress evaluation/ {DRIVE_BASE_PATH}/evaluation/ 2>/dev/null || cp -r evaluation/* {DRIVE_BASE_PATH}/evaluation/ 2>/dev/null || true\n",
    "        print(f\"✓ Backed up evaluation directory to {DRIVE_BASE_PATH}/evaluation/\")\n",
    "    \n",
    "    # Copy tensorboard logs if they exist\n",
    "    if os.path.exists('runs'):\n",
    "        os.makedirs(f\"{DRIVE_BASE_PATH}/tensorboard\", exist_ok=True)\n",
    "        !rsync -av --progress runs/ {DRIVE_BASE_PATH}/tensorboard/ 2>/dev/null || cp -r runs/* {DRIVE_BASE_PATH}/tensorboard/ 2>/dev/null || true\n",
    "        print(f\"✓ Backed up tensorboard logs to {DRIVE_BASE_PATH}/tensorboard/\")\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"BACKUP COMPLETED - ALL TRAINING ARTIFACTS SAVED\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "    \n",
    "    # List all backed up directories\n",
    "    print(\"Contents of Drive backup directory:\")\n",
    "    !find {DRIVE_BASE_PATH} -type d | sort\n",
    "\n",
    "# Register this function to be called when the user runs it explicitly\n",
    "print(\"Final backup function defined. Run 'backup_all_training_artifacts()' at any time to ensure all\")\n",
    "print(\"training artifacts are completely backed up to Google Drive, or when you're finishing your work.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up comprehensive logging and periodic saves to Google Drive\n",
    "import time\n",
    "import threading\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Create a log directory in Drive\n",
    "DRIVE_LOG_PATH = f\"{DRIVE_BASE_PATH}/logs\"\n",
    "!mkdir -p {DRIVE_LOG_PATH}\n",
    "\n",
    "# Set up logging to both console and file\n",
    "log_file = f\"{DRIVE_LOG_PATH}/training_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Don't override print - just use logging directly\n",
    "logging.info(f\"Logging enabled to {log_file}\")\n",
    "print(f\"Logging enabled to {log_file}\")\n",
    "\n",
    "# Function to save checkpoints to Google Drive\n",
    "def save_checkpoint_periodically(interval=900):  # 900 seconds = 15 minutes\n",
    "    while True:\n",
    "        time.sleep(interval)\n",
    "        timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f\"\\n[{timestamp}] Saving checkpoint to Google Drive...\")\n",
    "        logging.info(f\"[{timestamp}] Saving checkpoint to Google Drive...\")\n",
    "        \n",
    "        # Ensure directories exist\n",
    "        !mkdir -p {DRIVE_BASE_PATH}/models 2>/dev/null || true\n",
    "        !mkdir -p {DRIVE_BASE_PATH}/datasets 2>/dev/null || true\n",
    "        !mkdir -p {DRIVE_BASE_PATH}/logs 2>/dev/null || true\n",
    "        \n",
    "        # Copy training state file to know where we left off\n",
    "        training_state = {}\n",
    "        if os.path.exists('output/llama3_reasoning'):\n",
    "            # Check for training state files\n",
    "            if os.path.exists('output/llama3_reasoning/trainer_state.json'):\n",
    "                !cp output/llama3_reasoning/trainer_state.json {DRIVE_BASE_PATH}/models/\n",
    "                print(f\"  - Saved trainer state file\")\n",
    "                logging.info(f\"  - Saved trainer state file\")\n",
    "            \n",
    "            # Check for checkpoint directories\n",
    "            checkpoints = !ls -d output/llama3_reasoning/checkpoint-* 2>/dev/null || true\n",
    "            if checkpoints:\n",
    "                for checkpoint in checkpoints:\n",
    "                    checkpoint_name = os.path.basename(checkpoint)\n",
    "                    checkpoint_drive_path = f\"{DRIVE_BASE_PATH}/models/{checkpoint_name}\"\n",
    "                    # Only copy if it doesn't exist or is newer\n",
    "                    if not os.path.exists(checkpoint_drive_path):\n",
    "                        !mkdir -p {checkpoint_drive_path}\n",
    "                        !cp -r {checkpoint}/* {checkpoint_drive_path}/\n",
    "                        print(f\"  - Saved new checkpoint: {checkpoint_name}\")\n",
    "                        logging.info(f\"  - Saved new checkpoint: {checkpoint_name}\")\n",
    "            \n",
    "            # Check for adapter model\n",
    "            if os.path.exists('output/llama3_reasoning/adapter_model'):\n",
    "                adapter_drive_path = f\"{DRIVE_BASE_PATH}/models/adapter_model\"\n",
    "                !mkdir -p {adapter_drive_path}\n",
    "                !cp -r output/llama3_reasoning/adapter_model/* {adapter_drive_path}/\n",
    "                print(f\"  - Saved adapter model\")\n",
    "                logging.info(f\"  - Saved adapter model\")\n",
    "        \n",
    "        # Save any processed datasets\n",
    "        datasets = !ls -d data/*_processed 2>/dev/null || true\n",
    "        for dataset in datasets:\n",
    "            dataset_name = os.path.basename(dataset)\n",
    "            dataset_drive_path = f\"{DRIVE_BASE_PATH}/datasets/{dataset_name}\"\n",
    "            if not os.path.exists(dataset_drive_path):\n",
    "                !mkdir -p {dataset_drive_path}\n",
    "                !cp -r {dataset}/* {dataset_drive_path}/\n",
    "                print(f\"  - Saved dataset: {dataset_name}\")\n",
    "                logging.info(f\"  - Saved dataset: {dataset_name}\")\n",
    "                \n",
    "        # Update the log file (copy the most recent version)\n",
    "        !cp {log_file} {DRIVE_LOG_PATH}/\n",
    "        \n",
    "        print(f\"[{timestamp}] Checkpoint save completed\")\n",
    "        logging.info(f\"[{timestamp}] Checkpoint save completed\")\n",
    "\n",
    "# Start the checkpoint thread\n",
    "checkpoint_thread = threading.Thread(target=save_checkpoint_periodically, daemon=True)\n",
    "checkpoint_thread.start()\n",
    "print(f\"Automatic checkpointing to Drive enabled (every 15 minutes)\")\n",
    "logging.info(f\"Automatic checkpointing to Drive enabled (every 15 minutes)\")\n",
    "print(f\"All outputs will persist in: {DRIVE_BASE_PATH}\")\n",
    "logging.info(f\"All outputs will persist in: {DRIVE_BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Drive-Integrated Configuration\n",
    "\n",
    "Update the configuration to save outputs to Google Drive and disable Flash Attention to avoid errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update training config to save to Drive and optimize memory usage\n",
    "import yaml\n",
    "\n",
    "with open('configs/llama3_reasoning.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update output directory to use our Drive path\n",
    "config['training']['output_dir'] = DRIVE_MODEL_PATH\n",
    "\n",
    "# Disable Flash Attention to avoid errors in Colab\n",
    "if 'model' in config and 'use_flash_attention' in config['model']:\n",
    "    config['model']['use_flash_attention'] = False\n",
    "    print(\"Flash Attention disabled to avoid errors\")\n",
    "\n",
    "# Optimize for Colab memory constraints\n",
    "print(\"Optimizing training configuration for Colab memory constraints...\")\n",
    "# Reduce batch size and optimize memory usage\n",
    "if 'training' in config:\n",
    "    # Reduce batch sizes\n",
    "    config['training']['per_device_train_batch_size'] = 2\n",
    "    config['training']['per_device_eval_batch_size'] = 2\n",
    "    # Increase gradient accumulation to maintain effective batch size\n",
    "    config['training']['gradient_accumulation_steps'] = 16\n",
    "    # Reduce dataloader workers to avoid shared memory issues\n",
    "    config['training']['dataloader_num_workers'] = 1\n",
    "    # Enable mixed precision\n",
    "    config['training']['fp16'] = True\n",
    "    # Disable torch compilation which can use more memory\n",
    "    if 'torch_compile' in config['training']:\n",
    "        config['training']['torch_compile'] = False\n",
    "    print(\"Training hyperparameters adjusted for memory efficiency\")\n",
    "\n",
    "# Reduce sequence length if needed\n",
    "if 'dataset' in config and 'max_seq_length' in config['dataset']:\n",
    "    original_length = config['dataset']['max_seq_length']\n",
    "    if original_length > 1024:\n",
    "        config['dataset']['max_seq_length'] = 1024\n",
    "        print(f\"Reduced sequence length from {original_length} to {config['dataset']['max_seq_length']} tokens\")\n",
    "\n",
    "# Save updated config\n",
    "with open('configs/llama3_reasoning_drive.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(f\"Updated config saved to configs/llama3_reasoning_drive.yaml with output_dir={DRIVE_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Authenticate and Process Data\n",
    "\n",
    "Authenticate with Hugging Face to access the gated Llama 3 model, then process the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Hugging Face\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Replace with your actual token\n",
    "HF_TOKEN = \"your_huggingface_token_here\"  \n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# Set environment variable for other libraries\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a smaller subset of the dataset for faster training\n",
    "from datasets import load_from_disk, DatasetDict, Dataset\n",
    "\n",
    "DATASET_SUBSET_PERCENTAGE = 10  # Use only 10% of the data\n",
    "\n",
    "def create_dataset_subset(dataset_path, subset_path, percentage=10):\n",
    "    \"\"\"Create a smaller subset of a dataset for faster training\"\"\"\n",
    "    print(f\"Creating {percentage}% subset of dataset from {dataset_path}\")\n",
    "    \n",
    "    # Load the original dataset\n",
    "    try:\n",
    "        original_dataset = load_from_disk(dataset_path)\n",
    "        print(f\"Original dataset loaded with splits: {list(original_dataset.keys())}\")\n",
    "        \n",
    "        # Create a subset of each split\n",
    "        subset_dict = {}\n",
    "        for split_name, split_data in original_dataset.items():\n",
    "            original_size = len(split_data)\n",
    "            subset_size = max(int(original_size * percentage / 100), 100)  # At least 100 examples\n",
    "            \n",
    "            # Take a random sample of the specified percentage\n",
    "            subset = split_data.shuffle(seed=42).select(range(subset_size))\n",
    "            subset_dict[split_name] = subset\n",
    "            \n",
    "            print(f\"Split '{split_name}': {original_size} → {subset_size} examples ({percentage}%)\")\n",
    "        \n",
    "        # Create a new DatasetDict with the subset\n",
    "        subset_dataset = DatasetDict(subset_dict)\n",
    "        \n",
    "        # Save the subset to disk\n",
    "        subset_dataset.save_to_disk(subset_path)\n",
    "        print(f\"Subset saved to {subset_path}\")\n",
    "        \n",
    "        return subset_dataset\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dataset subset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define paths\n",
    "FULL_DATASET_PATH = DRIVE_DATASET_PATH\n",
    "SUBSET_DATASET_PATH = f\"{DRIVE_BASE_PATH}/datasets/natural_reasoning_subset_{DATASET_SUBSET_PERCENTAGE}pct\"\n",
    "\n",
    "# Check if subset already exists\n",
    "if os.path.exists(SUBSET_DATASET_PATH):\n",
    "    print(f\"Dataset subset already exists at {SUBSET_DATASET_PATH}\")\n",
    "    # Update the dataset path to use the subset\n",
    "    DRIVE_DATASET_PATH = SUBSET_DATASET_PATH\n",
    "else:\n",
    "    # First ensure the full dataset exists\n",
    "    if not os.path.exists(FULL_DATASET_PATH):\n",
    "        print(f\"Full dataset not found at {FULL_DATASET_PATH}. Will process it first.\")\n",
    "    else:\n",
    "        # Create the subset\n",
    "        subset = create_dataset_subset(FULL_DATASET_PATH, SUBSET_DATASET_PATH, DATASET_SUBSET_PERCENTAGE)\n",
    "        if subset is not None:\n",
    "            # Update the dataset path to use the subset\n",
    "            DRIVE_DATASET_PATH = SUBSET_DATASET_PATH\n",
    "            print(f\"✅ Now using dataset subset at: {DRIVE_DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the number of training epochs and sequence length for faster training\n",
    "import yaml\n",
    "\n",
    "with open('configs/llama3_reasoning_drive.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Reduce sequence length from 2048 to 1024 for faster training and less memory\n",
    "if 'dataset' in config and 'max_seq_length' in config['dataset']:\n",
    "    original_length = config['dataset']['max_seq_length']\n",
    "    if original_length > 1024:\n",
    "        config['dataset']['max_seq_length'] = 1024\n",
    "        print(f\"Reduced sequence length from {original_length} to 1024 tokens\")\n",
    "\n",
    "# Reduce number of epochs from 3 to 1\n",
    "if 'training' in config and 'num_train_epochs' in config['training']:\n",
    "    original_epochs = config['training']['num_train_epochs']\n",
    "    if original_epochs > 1:\n",
    "        config['training']['num_train_epochs'] = 1\n",
    "        print(f\"Reduced training epochs from {original_epochs} to 1\")\n",
    "\n",
    "# Save the updated config\n",
    "with open('configs/llama3_reasoning_lightweight.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(f\"Lightweight training config saved to configs/llama3_reasoning_lightweight.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset already exists in Drive\n",
    "import os\n",
    "from datasets import load_from_disk, DatasetDict\n",
    "\n",
    "if os.path.exists(DRIVE_DATASET_PATH):\n",
    "    print(f\"Dataset found at {DRIVE_DATASET_PATH}\")\n",
    "    \n",
    "    # Load the dataset to check for validation split\n",
    "    dataset = load_from_disk(DRIVE_DATASET_PATH)\n",
    "    \n",
    "    # Check if validation split exists\n",
    "    if 'validation' not in dataset.keys():\n",
    "        print(\"No validation split found in dataset. Creating validation split...\")\n",
    "        \n",
    "        # Create validation split (10% of train data)\n",
    "        if 'train' in dataset:\n",
    "            split_dataset = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "            \n",
    "            # Create new dataset with validation split\n",
    "            updated_dataset = DatasetDict({\n",
    "                'train': split_dataset['train'],\n",
    "                'validation': split_dataset['test']\n",
    "            })\n",
    "            \n",
    "            # Save the updated dataset back to the same location\n",
    "            updated_dataset.save_to_disk(DRIVE_DATASET_PATH)\n",
    "            print(f\"✅ Created validation split from train data. Updated dataset saved to {DRIVE_DATASET_PATH}\")\n",
    "            \n",
    "            # Update the dataset variable\n",
    "            dataset = updated_dataset\n",
    "    \n",
    "    # Create a symlink to local directory for easier access\n",
    "    !mkdir -p data\n",
    "    !ln -sf {DRIVE_DATASET_PATH} data/natural_reasoning_processed\n",
    "    print(f\"✅ Using dataset from Drive: {DRIVE_DATASET_PATH}\")\n",
    "    \n",
    "else:\n",
    "    # Process the dataset and save directly to Drive\n",
    "    print(f\"Processing dataset and saving to {DRIVE_DATASET_PATH}...\")\n",
    "    !python -m src.data_processors.reasoning_processor --config configs/llama3_reasoning.yaml --output_path {DRIVE_DATASET_PATH}\n",
    "    \n",
    "    # Check if validation split was created during processing\n",
    "    dataset = load_from_disk(DRIVE_DATASET_PATH)\n",
    "    if 'validation' not in dataset.keys():\n",
    "        print(\"No validation split was created during processing. Creating one now...\")\n",
    "        \n",
    "        # Create validation split (10% of train data)\n",
    "        if 'train' in dataset:\n",
    "            split_dataset = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "            \n",
    "            # Create new dataset with validation split\n",
    "            updated_dataset = DatasetDict({\n",
    "                'train': split_dataset['train'],\n",
    "                'validation': split_dataset['test']\n",
    "            })\n",
    "            \n",
    "            # Save the updated dataset back to the same location\n",
    "            updated_dataset.save_to_disk(DRIVE_DATASET_PATH)\n",
    "            print(f\"✅ Created validation split from train data. Updated dataset saved to {DRIVE_DATASET_PATH}\")\n",
    "    \n",
    "    # Create a symlink to local directory for easier access\n",
    "    !mkdir -p data\n",
    "    !ln -sf {DRIVE_DATASET_PATH} data/natural_reasoning_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the processed dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the processed dataset\n",
    "try:\n",
    "    dataset = load_from_disk(\"data/natural_reasoning_processed\")\n",
    "    \n",
    "    # Print info about the dataset\n",
    "    print(f\"Dataset splits: {dataset.keys()}\")\n",
    "    if 'train' in dataset:\n",
    "        print(f\"Train size: {len(dataset['train'])}\")\n",
    "    if 'validation' in dataset:\n",
    "        print(f\"Validation size: {len(dataset['validation'])}\")\n",
    "    \n",
    "    # See the first example\n",
    "    print(\"\\nExample data:\")\n",
    "    print(dataset[list(dataset.keys())[0]][0])\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Will attempt to process dataset again during training if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a heartbeat file to detect if Colab session disconnects\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Create a heartbeat directory\n",
    "HEARTBEAT_PATH = f\"{DRIVE_BASE_PATH}/heartbeat\"\n",
    "!mkdir -p {HEARTBEAT_PATH}\n",
    "\n",
    "# Write initial heartbeat file\n",
    "heartbeat_file = f\"{HEARTBEAT_PATH}/heartbeat.txt\"\n",
    "\n",
    "def update_heartbeat():\n",
    "    \"\"\"Update heartbeat file every minute to track if Colab is still running\"\"\"\n",
    "    while True:\n",
    "        # Write current timestamp to heartbeat file\n",
    "        with open(heartbeat_file, 'w') as f:\n",
    "            timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            f.write(f\"Last heartbeat: {timestamp}\\n\")\n",
    "            f.write(f\"If you're seeing this file, it means the Colab session was running at {timestamp}.\\n\")\n",
    "            f.write(f\"If this timestamp is old, the session likely disconnected at that time.\\n\")\n",
    "        \n",
    "        # Copy to Drive\n",
    "        !cp {heartbeat_file} {HEARTBEAT_PATH}/\n",
    "        \n",
    "        # Wait for 60 seconds\n",
    "        time.sleep(60)\n",
    "\n",
    "# Start heartbeat thread\n",
    "heartbeat_thread = threading.Thread(target=update_heartbeat, daemon=True)\n",
    "heartbeat_thread.start()\n",
    "\n",
    "print(f\"Heartbeat monitoring enabled - tracking session activity at {heartbeat_file}\")\n",
    "print(f\"If Colab disconnects, you can check when it happened by looking at this file in your Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightweight Llama 3 Training\n",
    "\n",
    "This section uses a small subset of the data and reduced training parameters to make Llama 3 fine-tuning feasible on Colab's resources. It:\n",
    "\n",
    "1. Creates a 10% subset of the original dataset\n",
    "2. Reduces maximum sequence length\n",
    "3. Decreases the number of training epochs\n",
    "4. Uses optimized memory settings\n",
    "\n",
    "This approach should complete in 3-4 hours instead of days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory before training\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache cleared\")\n",
    "    \n",
    "# Run garbage collection\n",
    "gc.collect()\n",
    "print(\"Garbage collection completed\")\n",
    "\n",
    "# Show current GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "    \n",
    "# Print current GPU usage\n",
    "!nvidia-smi | grep MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a modified config file with more aggressive memory optimizations\n",
    "import yaml\n",
    "\n",
    "# Load the existing drive config\n",
    "with open('configs/llama3_reasoning_drive.yaml', 'r') as f:\n",
    "    memory_config = yaml.safe_load(f)\n",
    "\n",
    "# Add more aggressive memory optimizations if needed\n",
    "memory_config['model']['load_in_4bit'] = True  # Ensure 4-bit quantization is enabled\n",
    "memory_config['model']['use_nested_quant'] = True  # Enable nested quantization for even more memory savings\n",
    "\n",
    "# Save as a separate config for low-memory environments\n",
    "with open('configs/llama3_reasoning_lowmem.yaml', 'w') as f:\n",
    "    yaml.dump(memory_config, f)\n",
    "\n",
    "print(\"Created low-memory configuration with aggressive memory optimizations\")\n",
    "print(\"If you still encounter memory issues, you can use this config instead:\")\n",
    "print(\"!python -m src.trainers.qlora_trainer configs/llama3_reasoning_lowmem.yaml --dataset_path {DRIVE_DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for existing training state and set up for resuming\n",
    "def check_for_resume_point():\n",
    "    \"\"\"Check if there's an existing training state to resume from\"\"\"\n",
    "    import os\n",
    "    import glob\n",
    "    import json\n",
    "    import re\n",
    "    import yaml\n",
    "    \n",
    "    # First check if the fine-tuned model already exists (complete training)\n",
    "    if os.path.exists(os.path.join(DRIVE_MODEL_PATH, \"adapter_model\")):\n",
    "        print(f\"✓ Fine-tuned model already exists at {DRIVE_MODEL_PATH}/adapter_model\")\n",
    "        print(\"Skipping training step. If you want to retrain, delete this directory from your Drive.\")\n",
    "        return True\n",
    "    \n",
    "    # If not complete, check for checkpoints to resume from\n",
    "    print(\"Looking for checkpoints to resume training...\")\n",
    "    checkpoints = glob.glob(f\"{DRIVE_MODEL_PATH}/checkpoint-*\")\n",
    "    \n",
    "    if checkpoints:\n",
    "        # Find the latest checkpoint by sorting (checkpoint numbers should be sequential)\n",
    "        checkpoints.sort(key=lambda x: int(re.search(r'checkpoint-(\\d+)', x).group(1)), reverse=True)\n",
    "        latest_checkpoint = checkpoints[0]\n",
    "        checkpoint_num = re.search(r'checkpoint-(\\d+)', latest_checkpoint).group(1)\n",
    "        \n",
    "        print(f\"✓ Found checkpoint: {latest_checkpoint}\")\n",
    "        \n",
    "        # Check if trainer state exists\n",
    "        trainer_state_path = os.path.join(DRIVE_MODEL_PATH, \"trainer_state.json\")\n",
    "        if os.path.exists(trainer_state_path):\n",
    "            try:\n",
    "                with open(trainer_state_path, 'r') as f:\n",
    "                    trainer_state = json.load(f)\n",
    "                total_steps = trainer_state.get('max_steps', 'unknown')\n",
    "                completed_steps = trainer_state.get('global_step', 0)\n",
    "                print(f\"✓ Training was at step {completed_steps}/{total_steps}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not parse trainer state: {e}\")\n",
    "        \n",
    "        # Create local output dir if needed\n",
    "        !mkdir -p output/llama3_reasoning\n",
    "        \n",
    "        # Copy checkpoint to local storage for use\n",
    "        local_checkpoint = f\"output/llama3_reasoning/checkpoint-{checkpoint_num}\"\n",
    "        if not os.path.exists(local_checkpoint):\n",
    "            print(f\"Copying checkpoint from Drive to local storage for resuming...\")\n",
    "            !mkdir -p {local_checkpoint}\n",
    "            !cp -r {latest_checkpoint}/* {local_checkpoint}/\n",
    "        \n",
    "        # Add resume flag to config\n",
    "        print(f\"Modifying config to resume from checkpoint...\")\n",
    "        with open('configs/llama3_reasoning_resume.yaml', 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        \n",
    "        # Point to local checkpoint for resuming\n",
    "        config['model']['adapter_name_or_path'] = local_checkpoint\n",
    "        \n",
    "        with open('configs/llama3_reasoning_resume.yaml', 'w') as f:\n",
    "            yaml.dump(config, f)\n",
    "            \n",
    "        print(f\"⏳ Resuming training from checkpoint-{checkpoint_num}...\")\n",
    "        print(f\"Model will continue saving to {DRIVE_MODEL_PATH}\")\n",
    "        \n",
    "        try:\n",
    "            print(\"Resuming with standard configuration...\")\n",
    "            !python -m src.trainers.qlora_trainer configs/llama3_reasoning_resume.yaml --dataset_path {DRIVE_DATASET_PATH}\n",
    "        except Exception as e:\n",
    "            print(f\"Resuming with standard config failed: {e}\")\n",
    "            print(\"Trying with more aggressive memory optimizations...\")\n",
    "            \n",
    "            # Update low memory config for resuming\n",
    "            with open('configs/llama3_reasoning_lowmem.yaml', 'r') as f:\n",
    "                low_config = yaml.safe_load(f)\n",
    "            \n",
    "            low_config['model']['adapter_name_or_path'] = local_checkpoint\n",
    "            \n",
    "            with open('configs/llama3_reasoning_lowmem_resume.yaml', 'w') as f:\n",
    "                yaml.dump(low_config, f)\n",
    "            \n",
    "            # Clear memory before retrying\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            !python -m src.trainers.qlora_trainer configs/llama3_reasoning_lowmem_resume.yaml --dataset_path {DRIVE_DATASET_PATH}\n",
    "        \n",
    "        return True  # Training was resumed\n",
    "    \n",
    "    return False  # No resumption point found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training process with the lightweight configuration\n",
    "print(\"Starting lightweight fine-tuning process (this should take 3-4 hours)...\")\n",
    "print(f\"Model will be saved to {DRIVE_MODEL_PATH}\")\n",
    "print(f\"Using 10% of the dataset and reduced parameters\")\n",
    "\n",
    "# Use our lightweight config\n",
    "!python -m src.trainers.qlora_trainer configs/llama3_reasoning_lightweight.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the final backup to ensure all training artifacts are saved\n",
    "print(\"Training completed! Performing final backup to ensure all artifacts are saved...\")\n",
    "backup_all_training_artifacts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training execution - checks for resume point or starts fresh\n",
    "if not check_for_resume_point():\n",
    "    print(\"No existing checkpoints found. Starting new training...\")\n",
    "    print(f\"Model will be saved to {DRIVE_MODEL_PATH}\")\n",
    "    \n",
    "    # Try with the regular drive config first, but if it fails, use the low memory config\n",
    "    try:\n",
    "        print(\"Using standard optimized configuration...\")\n",
    "        !python -m src.trainers.qlora_trainer configs/llama3_reasoning_drive.yaml --dataset_path {DRIVE_DATASET_PATH}\n",
    "    except Exception as e:\n",
    "        print(f\"Standard training failed with error: {e}\")\n",
    "        print(\"Trying with more aggressive memory optimizations...\")\n",
    "        # Clear memory before retrying\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        !python -m src.trainers.qlora_trainer configs/llama3_reasoning_lowmem.yaml --dataset_path {DRIVE_DATASET_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Models\n",
    "\n",
    "Compare the performance of the base model vs. the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Try to load actual results from evaluation\n",
    "results_path = os.path.join(DRIVE_EVAL_PATH, \"Meta-Llama-3-8B_results.txt\")\n",
    "finetuned_results = {\"accuracy\": 0.75}  # Default if file doesn't exist\n",
    "\n",
    "if os.path.exists(results_path):\n",
    "    with open(results_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"accuracy\"):\n",
    "                finetuned_results[\"accuracy\"] = float(line.split(\":\")[1].strip())\n",
    "    print(f\"Loaded actual evaluation results: {finetuned_results}\")\n",
    "else:\n",
    "    print(\"Using placeholder results - actual evaluation results not found\")\n",
    "\n",
    "# Base model results (placeholder - replace with actual if available)\n",
    "base_model_results = {\"accuracy\": 0.65}\n",
    "\n",
    "# Create comparison dataframe\n",
    "df = pd.DataFrame({\n",
    "    \"Model\": [\"Base Llama 3 8B\", \"Fine-tuned Llama 3 8B\"],\n",
    "    \"Accuracy\": [base_model_results[\"accuracy\"], finetuned_results[\"accuracy\"]]\n",
    "})\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = df.plot.bar(x=\"Model\", y=\"Accuracy\", rot=0)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.set_title(\"Reasoning Performance Comparison\")\n",
    "\n",
    "for i, v in enumerate(df[\"Accuracy\"]):\n",
    "    ax.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = os.path.join(DRIVE_EVAL_PATH, \"model_comparison.png\")\n",
    "plt.savefig(plot_path)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Comparison plot saved to {plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Package LoRA Adapter for Download\n",
    "\n",
    "Create a downloadable package of the adapter for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "adapter_path = os.path.join(DRIVE_MODEL_PATH, \"adapter_model\")\n",
    "\n",
    "if os.path.exists(adapter_path):\n",
    "    # Create export directory\n",
    "    !mkdir -p {DRIVE_ADAPTER_PATH}\n",
    "    \n",
    "    # Copy adapter files\n",
    "    !cp -r {adapter_path}/* {DRIVE_ADAPTER_PATH}/\n",
    "    \n",
    "    print(f\"Adapter exported to {DRIVE_ADAPTER_PATH}\")\n",
    "    \n",
    "    # Create a zip file for easy download\n",
    "    !cd {DRIVE_BASE_PATH} && zip -r lora_adapter.zip lora_adapter\n",
    "    print(f\"Adapter ZIP file created at {DRIVE_ADAPTER_ZIP}\")\n",
    "    \n",
    "    # Display file sizes\n",
    "    !du -h {DRIVE_ADAPTER_PATH} {DRIVE_ADAPTER_ZIP}\n",
    "else:\n",
    "    print(f\"Adapter not found at {adapter_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test the Fine-tuned Model\n",
    "\n",
    "Try out the fine-tuned model on custom reasoning questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the adapter config\n",
    "config = PeftConfig.from_pretrained(DRIVE_MODEL_PATH)\n",
    "\n",
    "# Load base model with authentication\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"eager\",  # Use eager implementation instead of flash attention\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# Load adapter model\n",
    "model = PeftModel.from_pretrained(base_model, DRIVE_MODEL_PATH, is_trainable=False)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.base_model_name_or_path, \n",
    "    trust_remote_code=True,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on some custom questions\n",
    "test_questions = [\n",
    "    \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly?\",\n",
    "    \"If no mammals can fly, and all bats can fly, what can we conclude about bats?\",\n",
    "    \"If all A are B, and all B are C, what can we conclude about the relationship between A and C?\"\n",
    "]\n",
    "\n",
    "# Create a file to store results\n",
    "test_results_path = os.path.join(DRIVE_EVAL_PATH, \"custom_test_results.txt\")\n",
    "with open(test_results_path, \"w\") as f:\n",
    "    for question in test_questions:\n",
    "        prompt = f\"Question: {question}\\n\\nAnswer: \"\n",
    "        result = pipe(prompt, return_full_text=False)[0][\"generated_text\"]\n",
    "        \n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {result}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Also write to file\n",
    "        f.write(f\"Question: {question}\\n\")\n",
    "        f.write(f\"Answer: {result}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "\n",
    "print(f\"Test results also saved to {test_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyze Training Progress After Session Restart\n",
    "\n",
    "If your Colab session was disconnected and you've returned to a new session, run the cells below to get insights into what happened during the previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the heartbeat file to see when the previous session disconnected\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "print(f\"{'='*50}\")\n",
    "print(\"PREVIOUS SESSION ANALYSIS\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "# Check if heartbeat file exists\n",
    "heartbeat_path = f\"{DRIVE_BASE_PATH}/heartbeat/heartbeat.txt\"\n",
    "if os.path.exists(heartbeat_path):\n",
    "    with open(heartbeat_path, 'r') as f:\n",
    "        heartbeat_content = f.read()\n",
    "    print(\"Last session heartbeat:\")\n",
    "    print(f\"{heartbeat_content}\\n\")\n",
    "else:\n",
    "    print(\"No heartbeat file found. Previous session may not have created one.\\n\")\n",
    "\n",
    "# Check for training state\n",
    "trainer_state_path = f\"{DRIVE_BASE_PATH}/models/trainer_state.json\"\n",
    "if os.path.exists(trainer_state_path):\n",
    "    try:\n",
    "        with open(trainer_state_path, 'r') as f:\n",
    "            trainer_state = json.load(f)\n",
    "        \n",
    "        # Extract key information\n",
    "        total_steps = trainer_state.get('max_steps', 'unknown')\n",
    "        completed_steps = trainer_state.get('global_step', 0)\n",
    "        last_log = trainer_state.get('log_history', [{}])[-1]\n",
    "        \n",
    "        # Calculate percentage complete\n",
    "        if isinstance(total_steps, int) and total_steps > 0:\n",
    "            pct_complete = (completed_steps / total_steps) * 100\n",
    "            print(f\"Training Progress: {completed_steps}/{total_steps} steps ({pct_complete:.1f}% complete)\")\n",
    "        else:\n",
    "            print(f\"Training Progress: {completed_steps}/{total_steps} steps\")\n",
    "        \n",
    "        # Show last metrics\n",
    "        if last_log:\n",
    "            print(\"\\nLast logged metrics:\")\n",
    "            for key, value in last_log.items():\n",
    "                if isinstance(value, float):\n",
    "                    print(f\"  - {key}: {value:.5f}\")\n",
    "                else:\n",
    "                    print(f\"  - {key}: {value}\")\n",
    "        \n",
    "        print(\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing trainer state: {e}\\n\")\n",
    "else:\n",
    "    print(\"No trainer state file found. Training may not have started or saved state.\\n\")\n",
    "\n",
    "# List checkpoints\n",
    "checkpoints = glob.glob(f\"{DRIVE_BASE_PATH}/models/checkpoint-*\")\n",
    "if checkpoints:\n",
    "    checkpoints.sort(key=lambda x: int(os.path.basename(x).split('-')[1]))\n",
    "    print(f\"Found {len(checkpoints)} checkpoints:\")\n",
    "    for i, checkpoint in enumerate(checkpoints[-5:], 1):  # Show last 5 checkpoints\n",
    "        checkpoint_num = os.path.basename(checkpoint).split('-')[1]\n",
    "        checkpoint_time = datetime.datetime.fromtimestamp(os.path.getmtime(checkpoint)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f\"  {i}. checkpoint-{checkpoint_num} (saved at {checkpoint_time})\")\n",
    "    \n",
    "    print(f\"\\nLatest checkpoint: {os.path.basename(checkpoints[-1])}\")\n",
    "    print(f\"To resume training from this checkpoint, continue with the notebook\\n\")\n",
    "else:\n",
    "    print(\"No checkpoints found in Drive. Training may not have saved any checkpoints yet.\\n\")\n",
    "\n",
    "# Check logs\n",
    "log_files = glob.glob(f\"{DRIVE_BASE_PATH}/logs/*.txt\")\n",
    "if log_files:\n",
    "    log_files.sort(key=os.path.getmtime, reverse=True)\n",
    "    latest_log = log_files[0]\n",
    "    log_time = datetime.datetime.fromtimestamp(os.path.getmtime(latest_log)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    print(f\"Latest log file: {os.path.basename(latest_log)} (last modified: {log_time})\")\n",
    "    print(\"Last 10 lines of log:\")\n",
    "    \n",
    "    try:\n",
    "        with open(latest_log, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines[-10:]:\n",
    "                print(f\"  {line.strip()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading log file: {e}\")\n",
    "else:\n",
    "    print(\"No log files found in Drive.\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"To resume training, continue with the notebook from Cell 3 (Setup)\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Access Your Outputs After Colab Shutdown\n",
    "\n",
    "All important files are now stored in your Google Drive and will persist even after the Colab session ends. Here's how to find them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n=== ALL OUTPUT LOCATIONS (in Google Drive) ===\\n\")\n",
    "print(f\"Root directory:     {DRIVE_BASE_PATH}\")\n",
    "print(f\"Processed Dataset:  {DRIVE_DATASET_PATH}\")\n",
    "print(f\"Fine-tuned Model:   {DRIVE_MODEL_PATH}\")\n",
    "print(f\"LoRA Adapter:       {DRIVE_ADAPTER_PATH}\")\n",
    "print(f\"LoRA Adapter ZIP:   {DRIVE_ADAPTER_ZIP}\")\n",
    "print(f\"Evaluation Results: {DRIVE_EVAL_PATH}\")\n",
    "\n",
    "# List all saved directories in Drive\n",
    "print(\"\\n=== DIRECTORIES CREATED IN GOOGLE DRIVE ===\\n\")\n",
    "!find {DRIVE_BASE_PATH} -type d | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a summary of what was created\n",
    "print(\"=== LLM Fine-tuning Summary ===\")\n",
    "print(f\"Dataset: {'✓' if os.path.exists(DRIVE_DATASET_PATH) else '✗'}\")\n",
    "print(f\"Trained Model: {'✓' if os.path.exists(DRIVE_MODEL_PATH) else '✗'}\")\n",
    "print(f\"LoRA Adapter: {'✓' if os.path.exists(DRIVE_ADAPTER_PATH) else '✗'}\")\n",
    "print(f\"Evaluation Results: {'✓' if os.path.exists(DRIVE_EVAL_PATH) else '✗'}\")\n",
    "print(\"\\nAll files are stored in your Google Drive and will be available after this Colab session ends.\")\n",
    "print(f\"\\nTo use a different Drive location for future runs, just change the DRIVE_OUTPUT_DIR variable at the beginning of the notebook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}