{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama 3 for Reasoning with QLoRA (Drive-Integrated)\n",
    "\n",
    "This notebook demonstrates fine-tuning Llama 3 8B using QLoRA for improved reasoning capabilities, with all data saved to Google Drive for persistence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, check GPU availability, install dependencies, and set up persistent storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure output directory in Google Drive (change this to your preferred location)\n",
    "DRIVE_OUTPUT_DIR = \"llm-trainer-output\"  # Will be created under /content/drive/MyDrive/\n",
    "\n",
    "# You can change this to a different name if you want, e.g.,:\n",
    "# DRIVE_OUTPUT_DIR = \"my-llm-experiments/llama3-reasoning-1\"\n",
    "\n",
    "# Full path to the output directory\n",
    "DRIVE_BASE_PATH = f\"/content/drive/MyDrive/{DRIVE_OUTPUT_DIR}\"\n",
    "\n",
    "# Specific paths for different components\n",
    "DRIVE_DATASET_PATH = f\"{DRIVE_BASE_PATH}/datasets/natural_reasoning_processed\"\n",
    "DRIVE_MODEL_PATH = f\"{DRIVE_BASE_PATH}/models/llama3_reasoning\"\n",
    "DRIVE_EVAL_PATH = f\"{DRIVE_BASE_PATH}/evaluation/reasoning_results\"\n",
    "DRIVE_ADAPTER_PATH = f\"{DRIVE_BASE_PATH}/lora_adapter\"\n",
    "DRIVE_ADAPTER_ZIP = f\"{DRIVE_BASE_PATH}/lora_adapter.zip\"\n",
    "\n",
    "print(f\"All outputs will be saved to Google Drive under: {DRIVE_BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directories in Drive\n",
    "!mkdir -p {DRIVE_BASE_PATH}/datasets\n",
    "!mkdir -p {DRIVE_BASE_PATH}/models\n",
    "!mkdir -p {DRIVE_BASE_PATH}/evaluation\n",
    "\n",
    "print(f\"Created directories in Google Drive at: {DRIVE_BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/vmm/llm-trainer.git\n",
    "%cd llm-trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix module import issues\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check and fix the working directory\n",
    "if not os.path.exists('src'):\n",
    "    # If we're not in the repo root, try to find it\n",
    "    if os.path.exists('llm-trainer'):\n",
    "        %cd llm-trainer\n",
    "    else:\n",
    "        # If we can't find it, raise an error\n",
    "        raise FileNotFoundError(\"Cannot find repository root directory with 'src' folder\")\n",
    "\n",
    "# Add the current directory to Python's path\n",
    "sys.path.append('.')\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Python path includes current directory: {'./' in sys.path or '.' in sys.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up periodic saves to Google Drive\n",
    "import time\n",
    "import threading\n",
    "\n",
    "def save_checkpoint_periodically(interval=1800):  # 1800 seconds = 30 minutes\n",
    "    while True:\n",
    "        time.sleep(interval)\n",
    "        print(\"\\nSaving checkpoint to Google Drive...\")\n",
    "        # Synchronize any files that might have changed\n",
    "        !mkdir -p output 2>/dev/null || true\n",
    "        !cp -r output/* {DRIVE_BASE_PATH}/models/ 2>/dev/null || true\n",
    "        !cp -r data/* {DRIVE_BASE_PATH}/datasets/ 2>/dev/null || true\n",
    "        print(f\"Checkpoint saved at {time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Start the checkpoint thread\n",
    "checkpoint_thread = threading.Thread(target=save_checkpoint_periodically, daemon=True)\n",
    "checkpoint_thread.start()\n",
    "print(\"Automatic checkpointing to Drive enabled (every 30 minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Drive-Integrated Configuration\n",
    "\n",
    "Update the configuration to save outputs to Google Drive and disable Flash Attention to avoid errors."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Update training config to save to Drive and optimize memory usage\nimport yaml\n\nwith open('configs/llama3_reasoning.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\n# Update output directory to use our Drive path\nconfig['training']['output_dir'] = DRIVE_MODEL_PATH\n\n# Disable Flash Attention to avoid errors in Colab\nif 'model' in config and 'use_flash_attention' in config['model']:\n    config['model']['use_flash_attention'] = False\n    print(\"Flash Attention disabled to avoid errors\")\n\n# Optimize for Colab memory constraints\nprint(\"Optimizing training configuration for Colab memory constraints...\")\n# Reduce batch size and optimize memory usage\nif 'training' in config:\n    # Reduce batch sizes\n    config['training']['per_device_train_batch_size'] = 2\n    config['training']['per_device_eval_batch_size'] = 2\n    # Increase gradient accumulation to maintain effective batch size\n    config['training']['gradient_accumulation_steps'] = 16\n    # Reduce dataloader workers to avoid shared memory issues\n    config['training']['dataloader_num_workers'] = 1\n    # Enable mixed precision\n    config['training']['fp16'] = True\n    # Disable torch compilation which can use more memory\n    if 'torch_compile' in config['training']:\n        config['training']['torch_compile'] = False\n    print(\"Training hyperparameters adjusted for memory efficiency\")\n\n# Reduce sequence length if needed\nif 'dataset' in config and 'max_seq_length' in config['dataset']:\n    if config['dataset']['max_seq_length'] > 1024:\n        original_length = config['dataset']['max_seq_length']\n        config['dataset']['max_seq_length'] = 1024\n        print(f\"Reduced sequence length from {original_length} to {config['dataset']['max_seq_length']} tokens\")\n\n# Save updated config\nwith open('configs/llama3_reasoning_drive.yaml', 'w') as f:\n    yaml.dump(config, f)\n\nprint(f\"Updated config saved to configs/llama3_reasoning_drive.yaml with output_dir={DRIVE_MODEL_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Authenticate and Process Data\n",
    "\n",
    "Authenticate with Hugging Face to access the gated Llama 3 model, then process the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Hugging Face\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Replace with your actual token\n",
    "HF_TOKEN = \"your_huggingface_token_here\"  \n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# Set environment variable for other libraries\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset already exists in Drive\n",
    "import os\n",
    "if os.path.exists(DRIVE_DATASET_PATH):\n",
    "    print(f\"Dataset already exists at {DRIVE_DATASET_PATH}\")\n",
    "    # Create a symlink to local directory for easier access\n",
    "    !mkdir -p data\n",
    "    !ln -sf {DRIVE_DATASET_PATH} data/natural_reasoning_processed\n",
    "else:\n",
    "    # Process the dataset and save directly to Drive\n",
    "    print(f\"Processing dataset and saving to {DRIVE_DATASET_PATH}...\")\n",
    "    !python -m src.data_processors.reasoning_processor --config configs/llama3_reasoning.yaml --output_path {DRIVE_DATASET_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the processed dataset\n",
    "try:\n",
    "    dataset = load_from_disk(DRIVE_DATASET_PATH)\n",
    "    \n",
    "    # Print info about the dataset\n",
    "    print(f\"Dataset splits: {dataset.keys()}\")\n",
    "    if 'train' in dataset:\n",
    "        print(f\"Train size: {len(dataset['train'])}\")\n",
    "    if 'validation' in dataset:\n",
    "        print(f\"Validation size: {len(dataset['validation'])}\")\n",
    "    \n",
    "    # See the first example\n",
    "    print(\"\\nExample data:\")\n",
    "    print(dataset[list(dataset.keys())[0]][0])\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Will attempt to process dataset again during training if needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning with QLoRA\n",
    "\n",
    "Fine-tune the Llama 3 model using QLoRA with all outputs saved to Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": "### Memory Management Tips\n\nBefore running the training process, here are some memory optimization strategies already applied:\n\n1. **Reduced Batch Size**: Set to 2 per device with increased gradient accumulation steps\n2. **Reduced Sequence Length**: Capped at 1024 tokens instead of 2048\n3. **Reduced Dataloader Workers**: Set to 1 to prevent shared memory errors\n4. **Mixed Precision Training**: Using FP16 to reduce memory usage\n\nIf you still encounter memory issues, try:\n\n- Clearing GPU memory before training: `torch.cuda.empty_cache()`\n- Restarting the Colab runtime before training\n- Further reducing batch size to 1\n- Disabling gradient checkpointing (but this may limit sequence length capacity)\n- Upgrading to Colab Pro+ for more memory\n- Trying a smaller model like Llama-3-8B-Instruct instead of the full Llama-3-8B"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Clean up memory before training\nimport gc\nimport torch\n\n# Clear CUDA cache\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    print(\"CUDA cache cleared\")\n    \n# Run garbage collection\ngc.collect()\nprint(\"Garbage collection completed\")\n\n# Show current GPU memory usage\nif torch.cuda.is_available():\n    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n    \n# Print current GPU usage\n!nvidia-smi | grep MiB"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Create a modified config file with more aggressive memory optimizations\nimport yaml\n\n# Load the existing drive config\nwith open('configs/llama3_reasoning_drive.yaml', 'r') as f:\n    memory_config = yaml.safe_load(f)\n\n# Add more aggressive memory optimizations if needed\nmemory_config['model']['load_in_4bit'] = True  # Ensure 4-bit quantization is enabled\nmemory_config['model']['use_nested_quant'] = True  # Enable nested quantization for even more memory savings\n\n# Save as a separate config for low-memory environments\nwith open('configs/llama3_reasoning_lowmem.yaml', 'w') as f:\n    yaml.dump(memory_config, f)\n\nprint(\"Created low-memory configuration with aggressive memory optimizations\")\nprint(\"If you still encounter memory issues, you can use this config instead:\")\nprint(\"!python -m src.trainers.qlora_trainer configs/llama3_reasoning_lowmem.yaml --dataset_path {DRIVE_DATASET_PATH}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Check if fine-tuned model already exists\nimport os\nif os.path.exists(os.path.join(DRIVE_MODEL_PATH, \"adapter_model\")):\n    print(f\"Fine-tuned model already exists at {DRIVE_MODEL_PATH}/adapter_model\")\n    print(\"Skipping training step. If you want to retrain, delete this directory from your Drive.\")\nelse:\n    # Fine-tune the model\n    print(f\"Starting fine-tuning process (this may take several hours)...\")\n    print(f\"Model will be saved to {DRIVE_MODEL_PATH}\")\n    \n    # Try with the regular drive config first, but if it fails, use the low memory config\n    try:\n        print(\"Using standard optimized configuration...\")\n        !python -m src.trainers.qlora_trainer configs/llama3_reasoning_drive.yaml --dataset_path {DRIVE_DATASET_PATH}\n    except Exception as e:\n        print(f\"Standard training failed with error: {e}\")\n        print(\"Trying with more aggressive memory optimizations...\")\n        # Clear memory before retrying\n        torch.cuda.empty_cache()\n        gc.collect()\n        !python -m src.trainers.qlora_trainer configs/llama3_reasoning_lowmem.yaml --dataset_path {DRIVE_DATASET_PATH}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Models\n",
    "\n",
    "Compare the performance of the base model vs. the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Try to load actual results from evaluation\n",
    "results_path = os.path.join(DRIVE_EVAL_PATH, \"Meta-Llama-3-8B_results.txt\")\n",
    "finetuned_results = {\"accuracy\": 0.75}  # Default if file doesn't exist\n",
    "\n",
    "if os.path.exists(results_path):\n",
    "    with open(results_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"accuracy\"):\n",
    "                finetuned_results[\"accuracy\"] = float(line.split(\":\")[1].strip())\n",
    "    print(f\"Loaded actual evaluation results: {finetuned_results}\")\n",
    "else:\n",
    "    print(\"Using placeholder results - actual evaluation results not found\")\n",
    "\n",
    "# Base model results (placeholder - replace with actual if available)\n",
    "base_model_results = {\"accuracy\": 0.65}\n",
    "\n",
    "# Create comparison dataframe\n",
    "df = pd.DataFrame({\n",
    "    \"Model\": [\"Base Llama 3 8B\", \"Fine-tuned Llama 3 8B\"],\n",
    "    \"Accuracy\": [base_model_results[\"accuracy\"], finetuned_results[\"accuracy\"]]\n",
    "})\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = df.plot.bar(x=\"Model\", y=\"Accuracy\", rot=0)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.set_title(\"Reasoning Performance Comparison\")\n",
    "\n",
    "for i, v in enumerate(df[\"Accuracy\"]):\n",
    "    ax.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = os.path.join(DRIVE_EVAL_PATH, \"model_comparison.png\")\n",
    "plt.savefig(plot_path)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Comparison plot saved to {plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Package LoRA Adapter for Download\n",
    "\n",
    "Create a downloadable package of the adapter for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "adapter_path = os.path.join(DRIVE_MODEL_PATH, \"adapter_model\")\n",
    "\n",
    "if os.path.exists(adapter_path):\n",
    "    # Create export directory\n",
    "    !mkdir -p {DRIVE_ADAPTER_PATH}\n",
    "    \n",
    "    # Copy adapter files\n",
    "    !cp -r {adapter_path}/* {DRIVE_ADAPTER_PATH}/\n",
    "    \n",
    "    print(f\"Adapter exported to {DRIVE_ADAPTER_PATH}\")\n",
    "    \n",
    "    # Create a zip file for easy download\n",
    "    !cd {DRIVE_BASE_PATH} && zip -r lora_adapter.zip lora_adapter\n",
    "    print(f\"Adapter ZIP file created at {DRIVE_ADAPTER_ZIP}\")\n",
    "    \n",
    "    # Display file sizes\n",
    "    !du -h {DRIVE_ADAPTER_PATH} {DRIVE_ADAPTER_ZIP}\n",
    "else:\n",
    "    print(f\"Adapter not found at {adapter_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test the Fine-tuned Model\n",
    "\n",
    "Try out the fine-tuned model on custom reasoning questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the adapter config\n",
    "config = PeftConfig.from_pretrained(DRIVE_MODEL_PATH)\n",
    "\n",
    "# Load base model with authentication\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"eager\",  # Use eager implementation instead of flash attention\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# Load adapter model\n",
    "model = PeftModel.from_pretrained(base_model, DRIVE_MODEL_PATH, is_trainable=False)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.base_model_name_or_path, \n",
    "    trust_remote_code=True,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on some custom questions\n",
    "test_questions = [\n",
    "    \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly?\",\n",
    "    \"If no mammals can fly, and all bats can fly, what can we conclude about bats?\",\n",
    "    \"If all A are B, and all B are C, what can we conclude about the relationship between A and C?\"\n",
    "]\n",
    "\n",
    "# Create a file to store results\n",
    "test_results_path = os.path.join(DRIVE_EVAL_PATH, \"custom_test_results.txt\")\n",
    "with open(test_results_path, \"w\") as f:\n",
    "    for question in test_questions:\n",
    "        prompt = f\"Question: {question}\\n\\nAnswer: \"\n",
    "        result = pipe(prompt, return_full_text=False)[0][\"generated_text\"]\n",
    "        \n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {result}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Also write to file\n",
    "        f.write(f\"Question: {question}\\n\")\n",
    "        f.write(f\"Answer: {result}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "\n",
    "print(f\"Test results also saved to {test_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Access Your Outputs After Colab Shutdown\n",
    "\n",
    "All important files are now stored in your Google Drive and will persist even after the Colab session ends. Here's how to find them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n=== ALL OUTPUT LOCATIONS (in Google Drive) ===\\n\")\n",
    "print(f\"Root directory:     {DRIVE_BASE_PATH}\")\n",
    "print(f\"Processed Dataset:  {DRIVE_DATASET_PATH}\")\n",
    "print(f\"Fine-tuned Model:   {DRIVE_MODEL_PATH}\")\n",
    "print(f\"LoRA Adapter:       {DRIVE_ADAPTER_PATH}\")\n",
    "print(f\"LoRA Adapter ZIP:   {DRIVE_ADAPTER_ZIP}\")\n",
    "print(f\"Evaluation Results: {DRIVE_EVAL_PATH}\")\n",
    "\n",
    "# List all saved directories in Drive\n",
    "print(\"\\n=== DIRECTORIES CREATED IN GOOGLE DRIVE ===\\n\")\n",
    "!find {DRIVE_BASE_PATH} -type d | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a summary of what was created\n",
    "print(\"=== LLM Fine-tuning Summary ===\")\n",
    "print(f\"Dataset: {'✓' if os.path.exists(DRIVE_DATASET_PATH) else '✗'}\")\n",
    "print(f\"Trained Model: {'✓' if os.path.exists(DRIVE_MODEL_PATH) else '✗'}\")\n",
    "print(f\"LoRA Adapter: {'✓' if os.path.exists(DRIVE_ADAPTER_PATH) else '✗'}\")\n",
    "print(f\"Evaluation Results: {'✓' if os.path.exists(DRIVE_EVAL_PATH) else '✗'}\")\n",
    "print(\"\\nAll files are stored in your Google Drive and will be available after this Colab session ends.\")\n",
    "print(f\"\\nTo use a different Drive location for future runs, just change the DRIVE_OUTPUT_DIR variable at the beginning of the notebook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}