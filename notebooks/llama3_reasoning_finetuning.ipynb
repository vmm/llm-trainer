{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama 3 for Reasoning with QLoRA\n",
    "\n",
    "This notebook demonstrates fine-tuning Llama 3 8B using QLoRA for improved reasoning capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's check if we have a GPU available and install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/yourusername/llm-trainer.git\n",
    "%cd llm-trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing\n",
    "\n",
    "Process the Natural Reasoning dataset for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Process the reasoning dataset\n",
    "!python -m src.data_processors.reasoning_processor --config configs/llama3_reasoning.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tuning with QLoRA\n",
    "\n",
    "Fine-tune the Llama 3 model using QLoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up Hugging Face access token\n",
    "import os\n",
    "\n",
    "# Set your Hugging Face token for accessing the model\n",
    "os.environ[\"HF_TOKEN\"] = \"your_huggingface_token_here\"  # Replace with your actual token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Fine-tune the model\n",
    "!python -m src.trainers.qlora_trainer configs/llama3_reasoning.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "Evaluate the fine-tuned model on the LogiQA benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set output directory for evaluation results\n",
    "output_dir = \"./evaluation_results\"\n",
    "model_path = \"./output/llama3_reasoning\"  # Path to the trained model\n",
    "\n",
    "# Evaluate the model\n",
    "!python -m src.evaluators.reasoning_evaluator --config configs/llama3_reasoning.yaml \\\n",
    "    --model_path {model_path} \\\n",
    "    --output_dir {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Models\n",
    "\n",
    "Compare the performance of the base model vs. the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load evaluation results\n",
    "base_model_results = {\"accuracy\": 0.65}  # Replace with actual base model results\n",
    "finetuned_results = {\"accuracy\": 0.75}  # Replace with actual fine-tuned results\n",
    "\n",
    "# Create comparison dataframe\n",
    "df = pd.DataFrame({\n",
    "    \"Model\": [\"Base Llama 3 8B\", \"Fine-tuned Llama 3 8B\"],\n",
    "    \"Accuracy\": [base_model_results[\"accuracy\"], finetuned_results[\"accuracy\"]]\n",
    "})\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = df.plot.bar(x=\"Model\", y=\"Accuracy\", rot=0)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.set_title(\"Reasoning Performance Comparison\")\n",
    "\n",
    "for i, v in enumerate(df[\"Accuracy\"]):\n",
    "    ax.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"model_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export LoRA Adapter\n",
    "\n",
    "Export the fine-tuned LoRA adapter for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import shutil\n",
    "\n",
    "# Copy adapter weights to a specific directory for easy download\n",
    "adapter_path = \"./output/llama3_reasoning/adapter_model\"\n",
    "export_path = \"./lora_adapter\"\n",
    "\n",
    "if os.path.exists(adapter_path):\n",
    "    # Create export directory\n",
    "    os.makedirs(export_path, exist_ok=True)\n",
    "    \n",
    "    # Copy adapter files\n",
    "    for file in os.listdir(adapter_path):\n",
    "        shutil.copy(os.path.join(adapter_path, file), export_path)\n",
    "    \n",
    "    print(f\"Adapter exported to {export_path}\")\n",
    "else:\n",
    "    print(f\"Adapter not found at {adapter_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Fine-tuned Model\n",
    "\n",
    "Try out the fine-tuned model on a few custom reasoning questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the base model and adapter\n",
    "model_path = \"./output/llama3_reasoning\"\n",
    "\n",
    "# Load adapter config\n",
    "config = PeftConfig.from_pretrained(model_path)\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load adapter model\n",
    "model = PeftModel.from_pretrained(base_model, model_path, is_trainable=False)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test on some custom questions\n",
    "test_questions = [\n",
    "    \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly?\",\n",
    "    \"If no mammals can fly, and all bats can fly, what can we conclude about bats?\",\n",
    "    \"If all A are B, and all B are C, what can we conclude about the relationship between A and C?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    prompt = f\"Question: {question}\\n\\nAnswer: \"\n",
    "    result = pipe(prompt, return_full_text=False)[0][\"generated_text\"]\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}