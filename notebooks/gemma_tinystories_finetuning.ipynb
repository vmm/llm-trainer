{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightweight Fine-tuning: Gemma-2B on TinyStories\n",
    "\n",
    "This notebook demonstrates fine-tuning Google's Gemma-2B model on the TinyStories dataset using QLoRA. This is a lightweight example that should train quickly on most GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, check GPU availability and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/vmm/llm-trainer.git\n",
    "%cd llm-trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix module import issues\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check and fix the working directory\n",
    "if not os.path.exists('src'):\n",
    "    # If we're not in the repo root, try to find it\n",
    "    if os.path.exists('llm-trainer'):\n",
    "        %cd llm-trainer\n",
    "    else:\n",
    "        # If we can't find it, raise an error\n",
    "        raise FileNotFoundError(\"Cannot find repository root directory with 'src' folder\")\n",
    "\n",
    "# Add the current directory to Python's path\n",
    "sys.path.append('.')\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Python path includes current directory: {'./' in sys.path or '.' in sys.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Configure output directory in Google Drive (change this to your preferred location)\n",
    "DRIVE_OUTPUT_DIR = \"llm-trainer-output\"  # Will be created under /content/drive/MyDrive/\n",
    "\n",
    "# Full path to the output directory\n",
    "DRIVE_BASE_PATH = f\"/content/drive/MyDrive/{DRIVE_OUTPUT_DIR}\"\n",
    "\n",
    "# Specific paths for different components\n",
    "DRIVE_DATASET_PATH = f\"{DRIVE_BASE_PATH}/datasets/tinystories_processed\"\n",
    "DRIVE_MODEL_PATH = f\"{DRIVE_BASE_PATH}/models/gemma_tinystories\"\n",
    "DRIVE_EVAL_PATH = f\"{DRIVE_BASE_PATH}/evaluation/tinystories_results\"\n",
    "DRIVE_ADAPTER_PATH = f\"{DRIVE_BASE_PATH}/lora_adapter\"\n",
    "DRIVE_ADAPTER_ZIP = f\"{DRIVE_BASE_PATH}/gemma_tinystories_adapter.zip\"\n",
    "\n",
    "# Create project directories in Drive\n",
    "!mkdir -p {DRIVE_BASE_PATH}/datasets\n",
    "!mkdir -p {DRIVE_BASE_PATH}/models\n",
    "!mkdir -p {DRIVE_BASE_PATH}/evaluation\n",
    "!mkdir -p {DRIVE_BASE_PATH}/logs\n",
    "\n",
    "print(f\"All outputs will be saved to Google Drive under: {DRIVE_BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Authenticate with Hugging Face\n",
    "\n",
    "Authenticate to access the Gemma model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Hugging Face\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Replace with your actual token\n",
    "HF_TOKEN = \"your_huggingface_token_here\"  \n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# Set environment variable for other libraries\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging and checkpoint saving to Google Drive\n",
    "import time\n",
    "import threading\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Create a log directory in Drive\n",
    "DRIVE_LOG_PATH = f\"{DRIVE_BASE_PATH}/logs\"\n",
    "!mkdir -p {DRIVE_LOG_PATH}\n",
    "\n",
    "# Set up logging to both console and file\n",
    "log_file = f\"{DRIVE_LOG_PATH}/training_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Logging enabled to {log_file}\")\n",
    "logging.info(f\"Logging enabled to {log_file}\")\n",
    "\n",
    "# Function to save checkpoints to Google Drive\n",
    "def save_checkpoint_periodically(interval=300):  # 300 seconds = 5 minutes\n",
    "    while True:\n",
    "        time.sleep(interval)\n",
    "        timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f\"\\n[{timestamp}] Saving checkpoint to Google Drive...\")\n",
    "        logging.info(f\"[{timestamp}] Saving checkpoint to Google Drive...\")\n",
    "        \n",
    "        # Ensure directories exist\n",
    "        !mkdir -p {DRIVE_BASE_PATH}/models 2>/dev/null || true\n",
    "        !mkdir -p {DRIVE_BASE_PATH}/datasets 2>/dev/null || true\n",
    "        !mkdir -p {DRIVE_BASE_PATH}/logs 2>/dev/null || true\n",
    "        \n",
    "        # Copy training state file to know where we left off\n",
    "        if os.path.exists('output/gemma_tinystories'):\n",
    "            # Check for training state files\n",
    "            if os.path.exists('output/gemma_tinystories/trainer_state.json'):\n",
    "                !cp output/gemma_tinystories/trainer_state.json {DRIVE_MODEL_PATH}/\n",
    "                print(f\"  - Saved trainer state file\")\n",
    "                logging.info(f\"  - Saved trainer state file\")\n",
    "            \n",
    "            # Check for checkpoint directories\n",
    "            checkpoints = !ls -d output/gemma_tinystories/checkpoint-* 2>/dev/null || true\n",
    "            if checkpoints:\n",
    "                for checkpoint in checkpoints:\n",
    "                    checkpoint_name = os.path.basename(checkpoint)\n",
    "                    checkpoint_drive_path = f\"{DRIVE_MODEL_PATH}/{checkpoint_name}\"\n",
    "                    # Only copy if it doesn't exist or is newer\n",
    "                    if not os.path.exists(checkpoint_drive_path):\n",
    "                        !mkdir -p {checkpoint_drive_path}\n",
    "                        !cp -r {checkpoint}/* {checkpoint_drive_path}/\n",
    "                        print(f\"  - Saved new checkpoint: {checkpoint_name}\")\n",
    "                        logging.info(f\"  - Saved new checkpoint: {checkpoint_name}\")\n",
    "            \n",
    "            # Check for adapter model\n",
    "            if os.path.exists('output/gemma_tinystories/adapter_model'):\n",
    "                adapter_drive_path = f\"{DRIVE_MODEL_PATH}/adapter_model\"\n",
    "                !mkdir -p {adapter_drive_path}\n",
    "                !cp -r output/gemma_tinystories/adapter_model/* {adapter_drive_path}/\n",
    "                print(f\"  - Saved adapter model\")\n",
    "                logging.info(f\"  - Saved adapter model\")\n",
    "        \n",
    "        # Save any processed datasets\n",
    "        datasets = !ls -d data/*_processed 2>/dev/null || true\n",
    "        for dataset in datasets:\n",
    "            dataset_name = os.path.basename(dataset)\n",
    "            dataset_drive_path = f\"{DRIVE_BASE_PATH}/datasets/{dataset_name}\"\n",
    "            if not os.path.exists(dataset_drive_path):\n",
    "                !mkdir -p {dataset_drive_path}\n",
    "                !cp -r {dataset}/* {dataset_drive_path}/\n",
    "                print(f\"  - Saved dataset: {dataset_name}\")\n",
    "                logging.info(f\"  - Saved dataset: {dataset_name}\")\n",
    "                \n",
    "        # Update the log file (copy the most recent version)\n",
    "        !cp {log_file} {DRIVE_LOG_PATH}/\n",
    "        \n",
    "        print(f\"[{timestamp}] Checkpoint save completed\")\n",
    "        logging.info(f\"[{timestamp}] Checkpoint save completed\")\n",
    "\n",
    "# Start the checkpoint thread\n",
    "checkpoint_thread = threading.Thread(target=save_checkpoint_periodically, daemon=True)\n",
    "checkpoint_thread.start()\n",
    "print(f\"Automatic checkpointing to Drive enabled (every 5 minutes)\")\n",
    "logging.info(f\"Automatic checkpointing to Drive enabled (every 5 minutes)\")\n",
    "print(f\"All outputs will persist in: {DRIVE_BASE_PATH}\")\n",
    "logging.info(f\"All outputs will persist in: {DRIVE_BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process the TinyStories Dataset\n",
    "\n",
    "Process the dataset and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset already exists in Drive\n",
    "import os\n",
    "from datasets import load_from_disk, DatasetDict\n",
    "\n",
    "if os.path.exists(DRIVE_DATASET_PATH):\n",
    "    print(f\"Dataset found at {DRIVE_DATASET_PATH}\")\n",
    "    \n",
    "    # Load the dataset to check for validation split\n",
    "    dataset = load_from_disk(DRIVE_DATASET_PATH)\n",
    "    \n",
    "    # Check if validation split exists\n",
    "    if 'validation' not in dataset.keys():\n",
    "        print(\"No validation split found in dataset. Creating validation split...\")\n",
    "        \n",
    "        # Create validation split (10% of train data)\n",
    "        if 'train' in dataset:\n",
    "            split_dataset = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "            \n",
    "            # Create new dataset with validation split\n",
    "            updated_dataset = DatasetDict({\n",
    "                'train': split_dataset['train'],\n",
    "                'validation': split_dataset['test']\n",
    "            })\n",
    "            \n",
    "            # Save the updated dataset back to the same location\n",
    "            updated_dataset.save_to_disk(DRIVE_DATASET_PATH)\n",
    "            print(f\"✅ Created validation split from train data. Updated dataset saved to {DRIVE_DATASET_PATH}\")\n",
    "            \n",
    "            # Update the dataset variable\n",
    "            dataset = updated_dataset\n",
    "    \n",
    "    # Create a symlink to local directory for easier access\n",
    "    !mkdir -p data\n",
    "    !ln -sf {DRIVE_DATASET_PATH} data/TinyStories_processed\n",
    "    print(f\"✅ Using dataset from Drive: {DRIVE_DATASET_PATH}\")\n",
    "    \n",
    "else:\n",
    "    # Process the dataset and save directly to Drive\n",
    "    print(f\"Processing dataset and saving to {DRIVE_DATASET_PATH}...\")\n",
    "    !python -m src.data_processors.tinystories_processor --config configs/gemma_tinystories.yaml --output_path {DRIVE_DATASET_PATH}\n",
    "    \n",
    "    # Check if validation split was created during processing\n",
    "    dataset = load_from_disk(DRIVE_DATASET_PATH)\n",
    "    if 'validation' not in dataset.keys():\n",
    "        print(\"No validation split was created during processing. Creating one now...\")\n",
    "        \n",
    "        # Create validation split (10% of train data)\n",
    "        if 'train' in dataset:\n",
    "            split_dataset = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "            \n",
    "            # Create new dataset with validation split\n",
    "            updated_dataset = DatasetDict({\n",
    "                'train': split_dataset['train'],\n",
    "                'validation': split_dataset['test']\n",
    "            })\n",
    "            \n",
    "            # Save the updated dataset back to the same location\n",
    "            updated_dataset.save_to_disk(DRIVE_DATASET_PATH)\n",
    "            print(f\"✅ Created validation split from train data. Updated dataset saved to {DRIVE_DATASET_PATH}\")\n",
    "    \n",
    "    # Create a symlink to local directory for easier access\n",
    "    !mkdir -p data\n",
    "    !ln -sf {DRIVE_DATASET_PATH} data/TinyStories_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update config to use Google Drive for output\n",
    "import yaml\n",
    "\n",
    "with open('configs/gemma_tinystories.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update output directory to use our Drive path\n",
    "config['training']['output_dir'] = DRIVE_MODEL_PATH\n",
    "\n",
    "# Save updated config\n",
    "with open('configs/gemma_tinystories_drive.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(f\"Updated config saved to configs/gemma_tinystories_drive.yaml with output_dir={DRIVE_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the processed dataset\n",
    "try:\n",
    "    dataset = load_from_disk(\"data/TinyStories_processed\")\n",
    "    \n",
    "    # Print info about the dataset\n",
    "    print(f\"Dataset splits: {dataset.keys()}\")\n",
    "    if 'train' in dataset:\n",
    "        print(f\"Train size: {len(dataset['train'])}\")\n",
    "    if 'validation' in dataset:\n",
    "        print(f\"Validation size: {len(dataset['validation'])}\")\n",
    "    \n",
    "    # See the first example\n",
    "    print(\"\\nExample data:\")\n",
    "    print(dataset[list(dataset.keys())[0]][0])\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up heartbeat monitoring to detect session disconnections\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Create a heartbeat directory\n",
    "HEARTBEAT_PATH = f\"{DRIVE_BASE_PATH}/heartbeat\"\n",
    "!mkdir -p {HEARTBEAT_PATH}\n",
    "\n",
    "# Write initial heartbeat file\n",
    "heartbeat_file = f\"{HEARTBEAT_PATH}/heartbeat.txt\"\n",
    "\n",
    "def update_heartbeat():\n",
    "    \"\"\"Update heartbeat file every minute to track if Colab is still running\"\"\"\n",
    "    while True:\n",
    "        # Write current timestamp to heartbeat file\n",
    "        with open(heartbeat_file, 'w') as f:\n",
    "            timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            f.write(f\"Last heartbeat: {timestamp}\\n\")\n",
    "            f.write(f\"If you're seeing this file, it means the Colab session was running at {timestamp}.\\n\")\n",
    "            f.write(f\"If this timestamp is old, the session likely disconnected at that time.\\n\")\n",
    "        \n",
    "        # Copy to Drive\n",
    "        !cp {heartbeat_file} {HEARTBEAT_PATH}/\n",
    "        \n",
    "        # Wait for 60 seconds\n",
    "        time.sleep(60)\n",
    "\n",
    "# Start heartbeat thread\n",
    "heartbeat_thread = threading.Thread(target=update_heartbeat, daemon=True)\n",
    "heartbeat_thread.start()\n",
    "\n",
    "print(f\"Heartbeat monitoring enabled - tracking session activity at {heartbeat_file}\")\n",
    "print(f\"If Colab disconnects, you can check when it happened by looking at this file in your Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-tune with QLoRA\n",
    "\n",
    "Fine-tune the Gemma-2B model using QLoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory before training\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache cleared\")\n",
    "    \n",
    "# Run garbage collection\n",
    "gc.collect()\n",
    "print(\"Garbage collection completed\")\n",
    "\n",
    "# Show current GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "    \n",
    "# Print current GPU usage\n",
    "!nvidia-smi | grep MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to backup all artifacts in case of manual shutdown\n",
    "def backup_all_training_artifacts():\n",
    "    \"\"\"\n",
    "    Perform a complete backup of all training artifacts to Google Drive.\n",
    "    Call this manually when you want to ensure everything is saved.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import time\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"PERFORMING FINAL BACKUP TO GOOGLE DRIVE\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "    \n",
    "    # Create all required directories\n",
    "    os.makedirs(f\"{DRIVE_BASE_PATH}/models\", exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_BASE_PATH}/datasets\", exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_BASE_PATH}/logs\", exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_BASE_PATH}/evaluation\", exist_ok=True)\n",
    "    \n",
    "    # Check for local output directory\n",
    "    if os.path.exists('output'):\n",
    "        # Copy all outputs (models, checkpoints, logs)\n",
    "        !rsync -av --progress output/ {DRIVE_BASE_PATH}/models/ 2>/dev/null || cp -r output/* {DRIVE_BASE_PATH}/models/ 2>/dev/null || true\n",
    "        print(f\"✓ Backed up output directory to {DRIVE_BASE_PATH}/models/\")\n",
    "    \n",
    "    # Check for local data directory \n",
    "    if os.path.exists('data'):\n",
    "        # Copy all datasets\n",
    "        !rsync -av --progress data/ {DRIVE_BASE_PATH}/datasets/ 2>/dev/null || cp -r data/* {DRIVE_BASE_PATH}/datasets/ 2>/dev/null || true\n",
    "        print(f\"✓ Backed up data directory to {DRIVE_BASE_PATH}/datasets/\")\n",
    "    \n",
    "    # Check for local logs\n",
    "    if os.path.exists('logs'):\n",
    "        # Copy all logs\n",
    "        !rsync -av --progress logs/ {DRIVE_BASE_PATH}/logs/ 2>/dev/null || cp -r logs/* {DRIVE_BASE_PATH}/logs/ 2>/dev/null || true\n",
    "        print(f\"✓ Backed up logs directory to {DRIVE_BASE_PATH}/logs/\")\n",
    "    \n",
    "    # Check for local evaluation results\n",
    "    if os.path.exists('evaluation'):\n",
    "        # Copy all evaluation results\n",
    "        !rsync -av --progress evaluation/ {DRIVE_BASE_PATH}/evaluation/ 2>/dev/null || cp -r evaluation/* {DRIVE_BASE_PATH}/evaluation/ 2>/dev/null || true\n",
    "        print(f\"✓ Backed up evaluation directory to {DRIVE_BASE_PATH}/evaluation/\")\n",
    "    \n",
    "    # Copy tensorboard logs if they exist\n",
    "    if os.path.exists('runs'):\n",
    "        os.makedirs(f\"{DRIVE_BASE_PATH}/tensorboard\", exist_ok=True)\n",
    "        !rsync -av --progress runs/ {DRIVE_BASE_PATH}/tensorboard/ 2>/dev/null || cp -r runs/* {DRIVE_BASE_PATH}/tensorboard/ 2>/dev/null || true\n",
    "        print(f\"✓ Backed up tensorboard logs to {DRIVE_BASE_PATH}/tensorboard/\")\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"BACKUP COMPLETED - ALL TRAINING ARTIFACTS SAVED\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "    \n",
    "    # List all backed up directories\n",
    "    print(\"Contents of Drive backup directory:\")\n",
    "    !find {DRIVE_BASE_PATH} -type d | sort\n",
    "\n",
    "# Register this function for manual use\n",
    "print(\"Run 'backup_all_training_artifacts()' at any time to ensure all artifacts are backed up to Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for existing training state and set up for resuming\n",
    "def check_for_resume_point():\n",
    "    \"\"\"Check if there's an existing training state to resume from\"\"\"\n",
    "    import os\n",
    "    import glob\n",
    "    import json\n",
    "    import re\n",
    "    import yaml\n",
    "    import torch\n",
    "    import gc\n",
    "\n",
    "    # First check if the fine-tuned model already exists (complete training)\n",
    "    if os.path.exists(os.path.join(DRIVE_MODEL_PATH, \"adapter_model\")):\n",
    "        print(f\"✓ Fine-tuned model already exists at {DRIVE_MODEL_PATH}/adapter_model\")\n",
    "        print(\"Skipping training step. If you want to retrain, delete this directory from your Drive.\")\n",
    "        return True\n",
    "    \n",
    "    # If not complete, check for checkpoints to resume from\n",
    "    print(\"Looking for checkpoints to resume training...\")\n",
    "    checkpoints = glob.glob(f\"{DRIVE_MODEL_PATH}/checkpoint-*\")\n",
    "    \n",
    "    if checkpoints:\n",
    "        # Find the latest checkpoint by sorting (checkpoint numbers should be sequential)\n",
    "        checkpoints.sort(key=lambda x: int(re.search(r'checkpoint-(\\d+)', x).group(1)), reverse=True)\n",
    "        latest_checkpoint = checkpoints[0]\n",
    "        checkpoint_num = re.search(r'checkpoint-(\\d+)', latest_checkpoint).group(1)\n",
    "        \n",
    "        print(f\"✓ Found checkpoint: {latest_checkpoint}\")\n",
    "        \n",
    "        # Check if trainer state exists\n",
    "        trainer_state_path = os.path.join(DRIVE_MODEL_PATH, \"trainer_state.json\")\n",
    "        if os.path.exists(trainer_state_path):\n",
    "            try:\n",
    "                with open(trainer_state_path, 'r') as f:\n",
    "                    trainer_state = json.load(f)\n",
    "                total_steps = trainer_state.get('max_steps', 'unknown')\n",
    "                completed_steps = trainer_state.get('global_step', 0)\n",
    "                print(f\"✓ Training was at step {completed_steps}/{total_steps}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not parse trainer state: {e}\")\n",
    "        \n",
    "        # Create local output dir if needed\n",
    "        !mkdir -p output/gemma_tinystories\n",
    "        \n",
    "        # Copy checkpoint to local storage for use\n",
    "        local_checkpoint = f\"output/gemma_tinystories/checkpoint-{checkpoint_num}\"\n",
    "        if not os.path.exists(local_checkpoint):\n",
    "            print(f\"Copying checkpoint from Drive to local storage for resuming...\")\n",
    "            !mkdir -p {local_checkpoint}\n",
    "            !cp -r {latest_checkpoint}/* {local_checkpoint}/\n",
    "        \n",
    "        # Add resume flag to config\n",
    "        print(f\"Modifying config to resume from checkpoint...\")\n",
    "        with open('configs/gemma_tinystories_drive.yaml', 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        \n",
    "        # Point to local checkpoint for resuming\n",
    "        config['model']['adapter_name_or_path'] = local_checkpoint\n",
    "        \n",
    "        with open('configs/gemma_tinystories_resume.yaml', 'w') as f:\n",
    "            yaml.dump(config, f)\n",
    "            \n",
    "        print(f\"⏳ Resuming training from checkpoint-{checkpoint_num}...\")\n",
    "        print(f\"Model will continue saving to {DRIVE_MODEL_PATH}\")\n",
    "        \n",
    "        try:\n",
    "            !python -m src.trainers.qlora_trainer configs/gemma_tinystories_resume.yaml\n",
    "        except Exception as e:\n",
    "            print(f\"Training failed: {e}\")\n",
    "            print(\"Trying again with clean memory...\")\n",
    "            \n",
    "            # Clear memory before retrying\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            !python -m src.trainers.qlora_trainer configs/gemma_tinystories_resume.yaml\n",
    "        \n",
    "        return True  # Training was resumed\n",
    "    \n",
    "    return False  # No resumption point found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training execution - checks for resume point or starts fresh\n",
    "if not check_for_resume_point():\n",
    "    print(\"No existing checkpoints found. Starting new training...\")\n",
    "    print(f\"Model will be saved to {DRIVE_MODEL_PATH}\")\n",
    "    \n",
    "    # Start fresh training\n",
    "    !python -m src.trainers.qlora_trainer configs/gemma_tinystories_drive.yaml\n",
    "\n",
    "# Backup everything when complete\n",
    "backup_all_training_artifacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Fine-tuned Model\n",
    "\n",
    "Try out the fine-tuned model by generating some stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test model from Google Drive\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the adapter config\n",
    "config = PeftConfig.from_pretrained(DRIVE_MODEL_PATH)\n",
    "\n",
    "# Load base model with authentication\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# Load adapter model\n",
    "model = PeftModel.from_pretrained(base_model, DRIVE_MODEL_PATH, is_trainable=False)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.base_model_name_or_path, \n",
    "    trust_remote_code=True,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with some story starters\n",
    "story_starters = [\n",
    "    \"Once upon a time, there was a little rabbit who\",\n",
    "    \"The small dog was very happy because\",\n",
    "    \"In a tiny house at the edge of the forest\"\n",
    "]\n",
    "\n",
    "for starter in story_starters:\n",
    "    print(f\"Prompt: {starter}\")\n",
    "    result = pipe(starter, return_full_text=True)[0][\"generated_text\"]\n",
    "    print(f\"Generated story:\\n{result}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare with Base Model\n",
    "\n",
    "Compare the fine-tuned model with the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model to compare\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,\n",
    "    trust_remote_code=True,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# Create a pipeline for the base model\n",
    "base_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare base model and fine-tuned model\n",
    "test_prompt = \"Once upon a time, there was a little rabbit who\"\n",
    "\n",
    "print(\"BASE MODEL OUTPUT:\")\n",
    "base_result = base_pipe(test_prompt, return_full_text=True)[0][\"generated_text\"]\n",
    "print(base_result)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "print(\"FINE-TUNED MODEL OUTPUT:\")\n",
    "ft_result = pipe(test_prompt, return_full_text=True)[0][\"generated_text\"]\n",
    "print(ft_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all output locations in Google Drive\n",
    "print(f\"\\n=== ALL OUTPUT LOCATIONS (in Google Drive) ===\\n\")\n",
    "print(f\"Root directory:     {DRIVE_BASE_PATH}\")\n",
    "print(f\"Processed Dataset:  {DRIVE_DATASET_PATH}\")\n",
    "print(f\"Fine-tuned Model:   {DRIVE_MODEL_PATH}\")\n",
    "print(f\"Evaluation Results: {DRIVE_EVAL_PATH}\")\n",
    "\n",
    "# List all saved directories in Drive\n",
    "print(\"\\n=== DIRECTORIES CREATED IN GOOGLE DRIVE ===\\n\")\n",
    "!find {DRIVE_BASE_PATH} -type d | sort\n",
    "\n",
    "# Display a summary of what was created\n",
    "print(\"\\n=== LLM Fine-tuning Summary ===\\n\")\n",
    "print(f\"Dataset: {'✓' if os.path.exists(DRIVE_DATASET_PATH) else '✗'}\")\n",
    "print(f\"Trained Model: {'✓' if os.path.exists(DRIVE_MODEL_PATH) else '✗'}\")\n",
    "print(f\"Adapter Files: {'✓' if os.path.exists(os.path.join(DRIVE_MODEL_PATH, 'adapter_model')) else '✗'}\")\n",
    "print(\"\\nAll files are stored in your Google Drive and will be available after this Colab session ends.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "This notebook demonstrated a lightweight fine-tuning of Gemma-2B on TinyStories data with Google Drive integration. Key highlights:\n",
    "\n",
    "1. Successfully fine-tuned Gemma-2B using QLoRA in 1-2 hours\n",
    "2. Used a small dataset subset for quick training\n",
    "3. Enabled proper validation during training\n",
    "4. Added automatic checkpointing to Google Drive every 5 minutes\n",
    "5. Provided checkpoint resumption for interrupted training\n",
    "6. Compared base model vs fine-tuned model outputs\n",
    "7. All results safely stored in Google Drive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}